## 今後の研究の方向性

- 分析メイン：データ分析で多様な状況に対応した予測手法の検討
- システムまで開発：ユーザ評価して発話タイミングを掴みやすいシステムを調査
    - 理由
        - 発話予測技術の精度がどのくらい必要なのか見通しを立てるため
          - どのくらい前の発話予測ができると良いのか？（直前 or 少し前）

## 今後のやること

- データ分析
    - 共通モデルの構築：LOPOによる評価
    - 予備動作だけでなく、非発話特徴による発話と非発話前の兆候を分類
    - 別会議パラメータを用いたデータ評価
    - OpenFaceからFacemeshに移行
    - 他者の音声特徴量：OpenSmile
    - 10回分のデータとか入れてしまう
    - 時系列的に予測結果を見てみる
    - 癖の分析
    - 多様な環境に対応可能な特徴量
- システム構築
    - google meetの拡張機能の実装
    - アンケート、発言数によるユーザ評価

## 学会でもらったコメント

- 対面と遠隔での発話予測では異なると感じた面は？（参加者の動きや、使用する特徴量など）
- → 遠隔だと環境が全く異なるので予測は難そうという感想
  - 前提面：対面は実際に同じ空間にいる相手を見て会話するが、遠隔はPC画面に向かって話をするので、頭部や視線が全く異なる。遠隔には対面にない、画面やマイクのオンオフもあるので、対面と同じ応用できるか明らかになってない。

- 実際システム化したら何秒ぐらい前に予測できれば良い？
  - 回答：トレードオフの関係になる可能性のため、今後検証してみたい
    - 直前の予測（1秒前くらい）だと7割~8割で予測可能
    - 少し前の予測（1秒以上前）だと精度が低くなる可能性

## システムの設計

### 手順

実装面
- raise-handのossをいじってみる
- facemeshの実装を入れられるか調整、方針を立てる
  - ダメそうならFacemeshを取り扱っている記事を探す
- 上記終了後、リスケ
- facemeshの実装に関するDocmentも読んで知見をまとめる

情報収集面
- 参考になりそうな記事を読む
  - https://github.com/kawamataryo/sync-raise-hand
  - https://zenn.dev/ryo_kawamata/articles/82871052dad5ff#%E6%8C%99%E6%89%8B%E3%83%9C%E3%82%BF%E3%83%B3%E3%81%AE%E6%8B%A1%E5%A4%A7%E3%82%A2%E3%83%8B%E3%83%A1%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3
  - https://ithink.ooo/
  - https://jpdebug.com/p/2818965
- 上記の記事の知見をまとめる

### 今後の実装方針

- バーの再描画の修正
- 欲求提示の実装
    - 色の明度の変化
    - リズムゲー
    - ゲージ
    - 数値
    - 代表で一人